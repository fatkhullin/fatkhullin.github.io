---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: default
---
<table>
  <tr>
    <td style="border:none">
      <a href="#about"><h3>About</h3></a>
    </td>
    <td style="border:none">
      <a href="#publ"><h3>Publications</h3></a>
    </td>
    <td style="border:none">
      <a href="#cont"><h3>Contact</h3></a>
    </td>
  </tr>
</table>

---
## About me {#about}

I am a second-year PhD student at ETH Zurich and ETH AI Center supervised by Prof. Niao He and Prof. Francesco Corman. My research interests include large-scale optimization, reinforcement learning and federated learning.

Previously, I had an honor to work with Prof. Boris Teodorovich Polyak and Prof. Peter Richtárik.

[![Google Scholar](/assets/google_scholar_logo.png){:width="5px"}](https://scholar.google.com/citations?user=UCOWHb4AAAAJ&hl=en)


## Publications {#publ}

- Ilyas Fatkhullin, Alexander Tyurin, Peter Richtárik. [Momentum Provably Improves Error Feedback!](https://arxiv.org/abs/2305.15155), NeurIPS 2023.

- Junchi Yang, Xiang Li, Ilyas Fatkhullin, Niao He. [Two Sides of One Coin: the Limits of Untuned SGD and the Power of Adaptive Methods](https://arxiv.org/abs/2305.12475), NeurIPS 2023.

- Anas Barakat, Ilyas Fatkhullin, Niao He. [Reinforcement Learning with General Utilities: Simpler Variance Reduction and Large State-Action Space](https://arxiv.org/abs/2306.01854), ICML 2023.

- Ilyas Fatkhullin, Anas Barakat, Anastasia Kireeva, Niao He. [Stochastic Policy Gradient Methods: Improved Sample Complexity for Fisher-non-degenerate Policies](https://arxiv.org/abs/2302.01734), ICML 2023.

- Ilyas Fatkhullin, Jalal Etesami, Niao He, Negar Kiyavash. [Sharp Analysis of Stochastic Optimization under Global Kurdyka-Łojasiewicz Inequality](https://arxiv.org/abs/2210.01748), NeurIPS 2022.

- Peter Richtárik, Igor Sokolov, Ilyas Fatkhullin, Elnur Gasanov, Eduard Gorbunov, Zhize Li. [3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation](https://arxiv.org/abs/2202.00998), ICML 2022 (spotlight).

- Ilyas Fatkhullin, Igor Sokolov, Eduard Gorbunov, Zhize Li, Peter Richtárik. [EF21 with Bells & Whistles: Practical Algorithmic Extensions of Modern Error Feedback](https://arxiv.org/abs/2110.03294), OptML workshop at NeurIPS 2021.

- Peter Richtárik, Igor Sokolov, Ilyas Fatkhullin. [EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback](https://arxiv.org/abs/2106.05203), NeurIPS 2021 (oral).

- Ilyas Fatkhullin, Boris Polyak. [Optimizing Static Linear Feedback: Gradient Method](https://arxiv.org/abs/2004.09875), SIAM Journal on Control and Optimization 59, 3887-3911 (2021).

- Boris Polyak, Ilyas Fatkhullin. [Use of Projective Coordinate Descent in the Fekete Problem](https://link.springer.com/article/10.1134/S0965542520050127), Comput. Math. and Math. Phys. 60, 795–807 (2020).


## Contact {#publ}

Current address: ETH AI Center, OAT X 14, Andreasstrasse 5, 8092 Zürich, Switzerland

E-mail: name.last_name(at)ai(dot)ethz(dot)ch
