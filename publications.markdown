---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: default
title: Publications
permalink: /Publications/
---
<table>
  <tr>
    <td style="border:none">
      <a href="/index"><h3>About</h3></a>
    </td>
    <td style="border:none">
      <a href="#publ"><h3>Publications</h3></a>
    </td>
    <td style="border:none">
      <a href="/contact"><h3>Contact</h3></a>
    </td>
  </tr>
</table>
---
# Publications {#publ}

- Ilyas Fatkhullin, Niao He, Yifan Hu. [Stochastic Optimization under Hidden Convexity](https://arxiv.org/abs/2401.00108), OptML workshop at NeurIPS 2023.

- Ilyas Fatkhullin, Alexander Tyurin, Peter Richtárik. [Momentum Provably Improves Error Feedback!](https://arxiv.org/abs/2305.15155), NeurIPS 2023.

- Junchi Yang, Xiang Li, Ilyas Fatkhullin, Niao He. [Two Sides of One Coin: the Limits of Untuned SGD and the Power of Adaptive Methods](https://arxiv.org/abs/2305.12475), NeurIPS 2023.

- Anas Barakat, Ilyas Fatkhullin, Niao He. [Reinforcement Learning with General Utilities: Simpler Variance Reduction and Large State-Action Space](https://arxiv.org/abs/2306.01854), ICML 2023.

- Ilyas Fatkhullin, Anas Barakat, Anastasia Kireeva, Niao He. [Stochastic Policy Gradient Methods: Improved Sample Complexity for Fisher-non-degenerate Policies](https://proceedings.mlr.press/v202/fatkhullin23a.html), ICML 2023.

- Ilyas Fatkhullin, Jalal Etesami, Niao He, Negar Kiyavash. [Sharp Analysis of Stochastic Optimization under Global Kurdyka-Łojasiewicz Inequality](https://arxiv.org/abs/2210.01748), NeurIPS 2022.

- Peter Richtárik, Igor Sokolov, Ilyas Fatkhullin, Elnur Gasanov, Eduard Gorbunov, Zhize Li. [3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation](https://arxiv.org/abs/2202.00998), ICML 2022 (spotlight).

- Ilyas Fatkhullin, Igor Sokolov, Eduard Gorbunov, Zhize Li, Peter Richtárik. [EF21 with Bells & Whistles: Practical Algorithmic Extensions of Modern Error Feedback](https://arxiv.org/abs/2110.03294), OptML workshop at NeurIPS 2021.

- Peter Richtárik, Igor Sokolov, Ilyas Fatkhullin. [EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback](https://arxiv.org/abs/2106.05203), NeurIPS 2021 (oral).

- Ilyas Fatkhullin, Boris Polyak. [Optimizing Static Linear Feedback: Gradient Method](https://arxiv.org/abs/2004.09875), SIAM Journal on Control and Optimization 59, 3887-3911 (2021).

- Boris Polyak, Ilyas Fatkhullin. [Use of Projective Coordinate Descent in the Fekete Problem](https://link.springer.com/article/10.1134/S0965542520050127), Comput. Math. and Math. Phys. 60, 795–807 (2020).
